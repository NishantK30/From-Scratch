{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"clean_weather.csv\")\n",
    "data = data.ffill()\n",
    "data['date'] = pd.to_datetime(data['Unnamed: 0'])\n",
    "data = data[['date','tmax','tmin','rain','tmax_tomorrow']]\n",
    "data = data.set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>rain</th>\n",
       "      <th>tmax_tomorrow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01</th>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02</th>\n",
       "      <td>52.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-03</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-04</th>\n",
       "      <td>53.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-05</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-22</th>\n",
       "      <td>62.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-23</th>\n",
       "      <td>67.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>66.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>70.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>62.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13509 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tmax  tmin  rain  tmax_tomorrow\n",
       "date                                       \n",
       "1970-01-01  60.0  35.0   0.0           52.0\n",
       "1970-01-02  52.0  39.0   0.0           52.0\n",
       "1970-01-03  52.0  35.0   0.0           53.0\n",
       "1970-01-04  53.0  36.0   0.0           52.0\n",
       "1970-01-05  52.0  35.0   0.0           50.0\n",
       "...          ...   ...   ...            ...\n",
       "2022-11-22  62.0  35.0   0.0           67.0\n",
       "2022-11-23  67.0  38.0   0.0           66.0\n",
       "2022-11-24  66.0  41.0   0.0           70.0\n",
       "2022-11-25  70.0  39.0   0.0           62.0\n",
       "2022-11-26  62.0  41.0   0.0           64.0\n",
       "\n",
       "[13509 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13509, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data_x,data_y, sequence_length):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.sequence_length+1\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of data\n",
    "        sequence_x = self.data_x[idx:idx + self.sequence_length]\n",
    "        sequence_y = self.data_y[idx:idx + self.sequence_length]\n",
    "        return sequence_x,sequence_y\n",
    "\n",
    "def transform(data_x,data_y,batch_size,sequence_len,device,dtype=torch.float32):\n",
    "    \n",
    "    data_tensor_x = torch.tensor(data_x,dtype=dtype,device=device)\n",
    "    data_tensor_y = torch.tensor(data_y,dtype=dtype,device=device)\n",
    "    dataset = TimeSeriesDataset(data_tensor_x,data_tensor_y,sequence_len)\n",
    "    \n",
    "    dataloader = DataLoader(dataset,batch_size,shuffle=False)\n",
    "    \n",
    "    return dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'date' is your date column and data is sorted by date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "predictors = [\"tmax\", \"tmin\", \"rain\"]\n",
    "target = \"tmax_tomorrow\"\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate split points\n",
    "total_rows = len(data)\n",
    "train_end = int(0.7 * total_rows)\n",
    "train_end = train_end - (train_end%32)\n",
    "valid_end = int(0.85 * total_rows)\n",
    "valid_end = valid_end-(valid_end%32)\n",
    "\n",
    "# Split the data\n",
    "X_train = data[predictors].iloc[:train_end]\n",
    "y_train = data[target].iloc[:train_end]\n",
    "\n",
    "X_valid = data[predictors].iloc[train_end:valid_end]\n",
    "y_valid = data[target].iloc[train_end:valid_end]\n",
    "\n",
    "X_test = data[predictors].iloc[valid_end:]\n",
    "y_test = data[target].iloc[valid_end:]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to a dataloader object with batch_size\n",
    "\n",
    "X_train_scaled = np.array(X_train_scaled)\n",
    "X_valid_scaled = np.array(X_valid_scaled)\n",
    "X_test_scaled = np.array(X_test_scaled)\n",
    "\n",
    "#We dont scale target data check on google for more info on why\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "y_valid = np.array(y_valid).reshape(-1,1)\n",
    "y_test = np.array(y_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train  (9440, 3)\n",
      "x_valid  (2016, 3)\n",
      "x_test (2053, 3)\n",
      "y_train (9440, 1)\n",
      "y_valid (2016, 1)\n",
      "y_test (2053, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train \",X_train_scaled.shape)\n",
    "print(\"x_valid \",X_valid_scaled.shape)\n",
    "print(\"x_test\",X_test_scaled.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"y_valid\",y_valid.shape)\n",
    "print(\"y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform(X_train_scaled,y_train,32,7,DEVICE)\n",
    "valid_data = transform(X_valid_scaled,y_valid,32,7,DEVICE)\n",
    "test_data = transform(X_test_scaled,y_test,32,7,DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train batch  torch.Size([32, 7, 3])    tensor([[-1.5982, -2.3133, -0.2672],\n",
      "        [-1.8390, -1.8620, -0.2672],\n",
      "        [-1.5982, -1.1097, -0.2672],\n",
      "        [-1.1165, -0.2070,  1.0794],\n",
      "        [-1.3573, -0.0565,  1.9771],\n",
      "        [-0.9961, -0.0565, -0.2672],\n",
      "        [-0.9961, -0.0565,  1.4721]], device='cuda:0')\n",
      "y_train batch  torch.Size([32, 7, 1])    tensor([[50.],\n",
      "        [52.],\n",
      "        [56.],\n",
      "        [54.],\n",
      "        [57.],\n",
      "        [57.],\n",
      "        [58.]], device='cuda:0')\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9434"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x, y in train_data:\n",
    "    print(\"x_train batch \",x.shape, \"  \",x[4])\n",
    "    print('y_train batch ',y.shape,\"  \",y[4])\n",
    "    break\n",
    "\n",
    "print(train_data.batch_size)\n",
    "len(train_data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size,output_size):\n",
    "        super(SimpleRNN,self).__init__()\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        k = 1/math.sqrt(hidden_size)\n",
    "        \n",
    "        #input to hidden layer\n",
    "        self.i2h = nn.Linear(input_size,hidden_size)\n",
    "        self.i2h.weight.data.uniform_(-k,k)\n",
    "        self.i2h.bias.data.uniform_(-k,k)\n",
    "        \n",
    "        #hidden to hidden layer\n",
    "        self.h2h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.h2h.weight.data.uniform_(-k,-k)\n",
    "        self.h2h.bias.data.uniform_(-k,k)\n",
    "        \n",
    "        #hidden to ouput layer\n",
    "        self.h2o = nn.Linear(hidden_size,output_size)\n",
    "        self.h2o.weight.data.uniform_(-k,k)\n",
    "        self.h2o.bias.data.uniform_(-k,k)\n",
    "        \n",
    "        #tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def init_hidden(self,batch_size,device=torch.device('cuda')):\n",
    "        return torch.zeros(batch_size,self.hidden_size,device=device,requires_grad=True)\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size,device=x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        current_hidden = hidden\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            #get current input\n",
    "            x_t = x[:,t,:]\n",
    "        \n",
    "            #combine input and hidden state and apply activation\n",
    "            combined = self.i2h(x_t) + self.h2h(current_hidden)\n",
    "            current_hidden = self.tanh(combined)\n",
    "            \n",
    "            #output layer\n",
    "            output = self.h2o(current_hidden)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        #stack outputs along sequence dimensions\n",
    "        outputs = torch.stack(outputs,dim=1)\n",
    "        \n",
    "        return outputs, current_hidden\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improved???\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImprovedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.2):\n",
    "        super(ImprovedRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.init_scale = 1.0 / math.sqrt(hidden_size)\n",
    "        \n",
    "        # Input to hidden layer\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        nn.init.xavier_uniform_(self.i2h.weight, gain=self.init_scale)\n",
    "        nn.init.zeros_(self.i2h.bias)\n",
    "        \n",
    "        # Hidden to hidden layer\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        # Orthogonal initialization for RNN hidden-to-hidden connection\n",
    "        nn.init.orthogonal_(self.h2h.weight, gain=self.init_scale)\n",
    "        nn.init.zeros_(self.h2h.bias)\n",
    "        \n",
    "        # Hidden to output layer\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        nn.init.xavier_uniform_(self.h2o.weight, gain=self.init_scale)\n",
    "        nn.init.zeros_(self.h2h.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Activation\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def init_hidden(self, batch_size, device=torch.device('cuda')):\n",
    "        # Initialize with small random values instead of zeros\n",
    "        return torch.randn(batch_size, self.hidden_size, device=device, \n",
    "                         requires_grad=True) * 0.1\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size, device=x.device)\n",
    "            \n",
    "        outputs = []\n",
    "        current_hidden = hidden\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Get current input\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Apply layer normalization and dropout to input\n",
    "            x_t = self.dropout(x_t)\n",
    "            \n",
    "            # Combine input and hidden state\n",
    "            i2h_out = self.i2h(x_t)\n",
    "            h2h_out = self.h2h(current_hidden)\n",
    "            #print(\"input shape: \",x.shape)\n",
    "            #print(\"i2h_out: \",i2h_out.shape,\" h2h : \",h2h_out.shape)\n",
    "            combined = i2h_out + h2h_out\n",
    "            \n",
    "            # Apply layer normalization before activation\n",
    "            combined = self.layer_norm(combined)\n",
    "            \n",
    "            # Apply activation and dropout\n",
    "            current_hidden = self.dropout(self.tanh(combined))\n",
    "            \n",
    "            # Output layer\n",
    "            output = self.h2o(current_hidden)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        # Stack outputs along sequence dimension\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        return outputs, current_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,valid_loader,num_epochs,learning_rate):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        #mean squared error\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "        \n",
    "        #to store metrics\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            hidden = None\n",
    "            \n",
    "            #Training loop \n",
    "            for batch_idx, (x_batch,y_batch) in enumerate(tqdm(train_loader)):\n",
    "                x_batch = x_batch.to(device) #shape (32,7,3)\n",
    "                y_batch = y_batch.to(device) #shape(32,7)\n",
    "                if x_batch.shape[0] != 32:\n",
    "                    continue\n",
    "                \n",
    "                #forward pass \n",
    "                outputs, hidden = model(x_batch,hidden) #outputs shape(32, 7, 1)\n",
    "                \n",
    "                #calculate loss\n",
    "                #print(\"ouputs: \",outputs.shape,\" y_batch \",y_batch.shape)\n",
    "                loss = criterion(outputs,y_batch)\n",
    "                \n",
    "                #backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Optional: Gradient clipping to prevent exploding gradients\n",
    "                nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # Detach hidden state to prevent backprop across batches\n",
    "                hidden = hidden.detach()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = total_train_loss/len(train_loader)\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            \n",
    "            #validation loop\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            hidden = None\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x_val,y_val in valid_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    if x_val.shape[0]!=32:\n",
    "                        continue\n",
    "                    \n",
    "                    outputs, hidden = model(x_val,hidden)\n",
    "                    # outputs = outputs.squeeze(-1)\n",
    "                    val_loss = criterion(outputs,y_val)\n",
    "                    total_val_loss += val_loss.item()\n",
    "                    \n",
    "                    hidden = hidden.detach()\n",
    "            \n",
    "            avg_val_loss = total_val_loss/len(valid_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            if(epoch%50==0):\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "                print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "                print(f'Validation Loss: {avg_val_loss:.4}')\n",
    "            \n",
    "        return train_losses, val_losses   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 129.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "Training Loss: 4170.5295\n",
      "Validation Loss: 4.233e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:03<00:00, 96.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "Training Loss: 3909.0547\n",
      "Validation Loss: 3.997e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 110.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "Training Loss: 3697.3962\n",
      "Validation Loss: 3.795e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 123.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "Training Loss: 3507.9080\n",
      "Validation Loss: 3.608e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 124.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "Training Loss: 3330.3430\n",
      "Validation Loss: 3.429e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 122.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "Training Loss: 3160.3635\n",
      "Validation Loss: 3.257e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 99.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "Training Loss: 2996.9891\n",
      "Validation Loss: 3.091e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 121.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "Training Loss: 2838.1688\n",
      "Validation Loss: 2.93e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 127.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n",
      "Training Loss: 2687.7174\n",
      "Validation Loss: 2.774e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:02<00:00, 127.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/10]\n",
      "Training Loss: 2539.2857\n",
      "Validation Loss: 2.624e+03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4170.529536877649,\n",
       "  3909.0547230865996,\n",
       "  3697.3961880958686,\n",
       "  3507.9080218816207,\n",
       "  3330.343027509269,\n",
       "  3160.363484010858,\n",
       "  2996.989063741393,\n",
       "  2838.1687793796345,\n",
       "  2687.717354260461,\n",
       "  2539.2856548761915],\n",
       " [4232.866075303819,\n",
       "  3996.9097338479664,\n",
       "  3795.470009455605,\n",
       "  3607.6668836805557,\n",
       "  3428.7284497457836,\n",
       "  3256.7952822730654,\n",
       "  3090.7939879402284,\n",
       "  2930.13137672061,\n",
       "  2774.451894608755,\n",
       "  2623.5345439608136])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImprovedRNN(input_size=3,hidden_size=4,output_size=1)\n",
    "\n",
    "train_model(model,train_data,valid_data,10,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
